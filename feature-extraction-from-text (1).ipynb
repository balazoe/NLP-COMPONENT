{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP ASSIGNMENT COMPONENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BALA MURUGAN - 225229150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "### Logloss for this competition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    :param actual: Array containing the actual target classes\n",
    "    :param predicted: Matrix with class predictions, one probability per class\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://spacy.io/\n",
    "spaCy is the best way to prepare text for deep learning. It interoperates seamlessly with TensorFlow, PyTorch, scikit-learn, Gensim and the rest of Python's awesome AI ecosystem. With spaCy, you can easily construct linguistically sophisticated statistical models for a variety of NLP problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#READING INPUT\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "* the process of breaking up the original text into components (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 90 DET det\n",
      "process 92 NOUN nsubj\n",
      ", 97 PUNCT punct\n",
      "however 86 ADV advmod\n",
      ", 97 PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(data[\"text\"][0])\n",
    "for token in doc[0:5]:\n",
    "    print(token.text, token.pos , token.pos_, token.dep_) # part of speach and syntax dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisation\n",
    "\n",
    "* in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This DET this\n",
      "process NOUN process\n",
      ", PUNCT ,\n",
      "however ADV however\n",
      ", PUNCT ,\n"
     ]
    }
   ],
   "source": [
    "for token in doc[0:5]:\n",
    "    print(token.text,  token.pos_, token.lemma_) # part of speach and syntax dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f9906896eb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=['Edgar Allen Poe', 'Mary Wollstonecraft Shelley', 'H.P. Lovecraft'], y=data['author'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   author_num  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           2  \n",
       "4           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['author_num'] = data[\"author\"].map({'EAP':0, 'HPL':1, 'MWS':2})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we map \"EAP\" to 0 \"HPL\" to 1 and \"MWS\" to 2 as it will be more convenient for our classifier. \n",
    "In other words we are just telling our computer that if classifier predicts 0 for the text then it means that it is preicting \"EAP\", if 1 then it means that it is predicting \"HPL\", if 2 then it means that it is predicting \"MWS\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta features\n",
    "\n",
    "* features that are extracted from the text like number of words, number of stop words, number of punctuations etc Number of words in the text\n",
    "* Number of unique words in the text\n",
    "* Number of characters in the text\n",
    "* Number of stopwords\n",
    "* Number of punctuations\n",
    "* Number of upper case words\n",
    "* Number of title case words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "print (stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of words in the text ##\n",
    "data[\"num_words\"] = data[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "test[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "## Number of unique words in the text ##\n",
    "data[\"num_unique_words\"] = data[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "## Number of characters in the text ##\n",
    "data[\"num_chars\"] = data[\"text\"].apply(lambda x: len(str(x)))\n",
    "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n",
    "## Number of stopwords in the text ##\n",
    "data[\"num_stopwords\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n",
    "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n",
    "## Number of punctuations in the text ##\n",
    "data[\"num_punctuations\"] =data['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "## Number of title case words in the text ##\n",
    "data[\"num_words_upper\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "## Number of title case words in the text ##\n",
    "data[\"num_words_title\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "## Max length of the words in the text ##\n",
    "data[\"max_word_len\"] = data[\"text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))\n",
    "test[\"max_word_len\"] = test[\"text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "        tokens = [tok for tok in tokens if tok not in stopwords and tok not in string.punctuation]\n",
    "        #tokens = [tok for tok in tokens if tok not in punctuations]\n",
    "        tokens = ' '.join(tokens)\n",
    "        texts.append(tokens)\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape:  (19579,)\n",
      "Processed 1000 out of 19579 documents.\n",
      "Processed 2000 out of 19579 documents.\n",
      "Processed 3000 out of 19579 documents.\n",
      "Processed 4000 out of 19579 documents.\n",
      "Processed 5000 out of 19579 documents.\n",
      "Processed 6000 out of 19579 documents.\n",
      "Processed 7000 out of 19579 documents.\n",
      "Processed 8000 out of 19579 documents.\n",
      "Processed 9000 out of 19579 documents.\n",
      "Processed 10000 out of 19579 documents.\n",
      "Processed 11000 out of 19579 documents.\n",
      "Processed 12000 out of 19579 documents.\n",
      "Processed 13000 out of 19579 documents.\n",
      "Processed 14000 out of 19579 documents.\n",
      "Processed 15000 out of 19579 documents.\n",
      "Processed 16000 out of 19579 documents.\n",
      "Processed 17000 out of 19579 documents.\n",
      "Processed 18000 out of 19579 documents.\n",
      "Processed 19000 out of 19579 documents.\n",
      "Cleaned up training data shape:  (19579,)\n"
     ]
    }
   ],
   "source": [
    "print('Original training data shape: ', data['text'].shape)\n",
    "data[\"text_cleaned\"]= cleanup_text(data['text'], logging=True)\n",
    "print('Cleaned up training data shape: ', data[\"text_cleaned\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape:  (8392,)\n",
      "Processed 1000 out of 8392 documents.\n",
      "Processed 2000 out of 8392 documents.\n",
      "Processed 3000 out of 8392 documents.\n",
      "Processed 4000 out of 8392 documents.\n",
      "Processed 5000 out of 8392 documents.\n",
      "Processed 6000 out of 8392 documents.\n",
      "Processed 7000 out of 8392 documents.\n",
      "Processed 8000 out of 8392 documents.\n",
      "Cleaned up training data shape:  (8392,)\n"
     ]
    }
   ],
   "source": [
    "print('Original training data shape: ', test['text'].shape)\n",
    "test[\"text_cleaned\"] = cleanup_text(test['text'], logging=True)\n",
    "print('Cleaned up training data shape: ', test[\"text_cleaned\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature extraction on Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data[\"num_unique_words_clenaed\"] = data[\"text_cleaned\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_unique_words_cleaned\"] = test[\"text_cleaned\"].apply(lambda x: len(set(str(x).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfADV(docs, logging=False):\n",
    "    numberOfADV = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.pos_ == 'ADP']\n",
    "        #tokens = [tok for tok in tokens if tok not in stopwords and tok not in string.punctuation]\n",
    "        #tokens = [tok for tok in tokens if tok not in punctuations]\n",
    "        #tokens = ' '.join(tokens)\n",
    "        \n",
    "        numberOfADV.append(len(tokens))\n",
    "    return pd.Series(numberOfADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 out of 19579 documents.\n",
      "Processed 2000 out of 19579 documents.\n",
      "Processed 3000 out of 19579 documents.\n",
      "Processed 4000 out of 19579 documents.\n",
      "Processed 5000 out of 19579 documents.\n",
      "Processed 6000 out of 19579 documents.\n",
      "Processed 7000 out of 19579 documents.\n",
      "Processed 8000 out of 19579 documents.\n",
      "Processed 9000 out of 19579 documents.\n",
      "Processed 10000 out of 19579 documents.\n",
      "Processed 11000 out of 19579 documents.\n",
      "Processed 12000 out of 19579 documents.\n",
      "Processed 13000 out of 19579 documents.\n",
      "Processed 14000 out of 19579 documents.\n",
      "Processed 15000 out of 19579 documents.\n",
      "Processed 16000 out of 19579 documents.\n",
      "Processed 17000 out of 19579 documents.\n",
      "Processed 18000 out of 19579 documents.\n",
      "Processed 19000 out of 19579 documents.\n"
     ]
    }
   ],
   "source": [
    "data[\"num_of_ADV\"] = numberOfADV(data['text_cleaned'], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 out of 8392 documents.\n",
      "Processed 2000 out of 8392 documents.\n",
      "Processed 3000 out of 8392 documents.\n",
      "Processed 4000 out of 8392 documents.\n",
      "Processed 5000 out of 8392 documents.\n",
      "Processed 6000 out of 8392 documents.\n",
      "Processed 7000 out of 8392 documents.\n",
      "Processed 8000 out of 8392 documents.\n"
     ]
    }
   ],
   "source": [
    "test[\"num_of_ADV\"] = numberOfADV(test['text_cleaned'], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfADJ(docs, logging=False):\n",
    "    numberOfADV = []\n",
    "    counter = 1\n",
    "    for doc in docs:\n",
    "        if counter % 1000 == 0 and logging:\n",
    "            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n",
    "        counter += 1\n",
    "        doc = nlp(doc, disable=['parser', 'ner'])\n",
    "        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.pos_ == 'ADJ']\n",
    "        \n",
    "        numberOfADV.append(len(tokens))\n",
    "    return pd.Series(numberOfADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 out of 19579 documents.\n",
      "Processed 2000 out of 19579 documents.\n",
      "Processed 3000 out of 19579 documents.\n",
      "Processed 4000 out of 19579 documents.\n",
      "Processed 5000 out of 19579 documents.\n",
      "Processed 6000 out of 19579 documents.\n",
      "Processed 7000 out of 19579 documents.\n",
      "Processed 8000 out of 19579 documents.\n",
      "Processed 9000 out of 19579 documents.\n",
      "Processed 10000 out of 19579 documents.\n",
      "Processed 11000 out of 19579 documents.\n",
      "Processed 12000 out of 19579 documents.\n",
      "Processed 13000 out of 19579 documents.\n",
      "Processed 14000 out of 19579 documents.\n",
      "Processed 15000 out of 19579 documents.\n",
      "Processed 16000 out of 19579 documents.\n",
      "Processed 17000 out of 19579 documents.\n",
      "Processed 18000 out of 19579 documents.\n",
      "Processed 19000 out of 19579 documents.\n"
     ]
    }
   ],
   "source": [
    "data[\"num_of_ADJ\"] = numberOfADJ(data['text_cleaned'], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 out of 8392 documents.\n",
      "Processed 2000 out of 8392 documents.\n",
      "Processed 3000 out of 8392 documents.\n",
      "Processed 4000 out of 8392 documents.\n",
      "Processed 5000 out of 8392 documents.\n",
      "Processed 6000 out of 8392 documents.\n",
      "Processed 7000 out of 8392 documents.\n",
      "Processed 8000 out of 8392 documents.\n"
     ]
    }
   ],
   "source": [
    "test[\"num_of_ADJ\"] = numberOfADJ(test['text_cleaned'], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>author_num</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>max_word_len</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>num_unique_words_clenaed</th>\n",
       "      <th>num_of_ADV</th>\n",
       "      <th>num_of_ADJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>process however afford means ascertain dimensi...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>never occur fumbling might mere mistake</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>left hand gold snuff box caper hill cut manner...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>lovely spring look windsor terrace sixteen fer...</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>find nothing else even gold superintendent aba...</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  This process, however, afforded me no means of...    EAP   \n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL   \n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP   \n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS   \n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL   \n",
       "\n",
       "   author_num  num_words  num_unique_words  num_chars  num_stopwords  \\\n",
       "0           0         41                35        231             19   \n",
       "1           1         14                14         71              8   \n",
       "2           0         36                32        200             16   \n",
       "3           2         34                32        206             13   \n",
       "4           1         27                25        174             11   \n",
       "\n",
       "   num_punctuations  num_words_upper  num_words_title  max_word_len  \\\n",
       "0                 7                2                3            12   \n",
       "1                 1                0                1             8   \n",
       "2                 5                0                1            13   \n",
       "3                 4                0                4             9   \n",
       "4                 4                0                2            14   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  process however afford means ascertain dimensi...   \n",
       "1            never occur fumbling might mere mistake   \n",
       "2  left hand gold snuff box caper hill cut manner...   \n",
       "3  lovely spring look windsor terrace sixteen fer...   \n",
       "4  find nothing else even gold superintendent aba...   \n",
       "\n",
       "   num_unique_words_clenaed  num_of_ADV  num_of_ADJ  \n",
       "0                        21           1           2  \n",
       "1                         6           0           1  \n",
       "2                        18           0           3  \n",
       "3                        20           1           5  \n",
       "4                        16           0           0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>max_word_len</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>num_unique_words_cleaned</th>\n",
       "      <th>num_of_ADV</th>\n",
       "      <th>num_of_ADJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>Still, as I urged our leaving Ireland with suc...</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>110</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>still urge leave ireland inquietude impatience...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>If a fire wanted fanning, it could readily be ...</td>\n",
       "      <td>62</td>\n",
       "      <td>49</td>\n",
       "      <td>330</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>fire want fan could readily fan newspaper gove...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>And when they had broken down the frail door t...</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>189</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>break frail door find two cleanly pick human s...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>While I was thinking how I should possibly man...</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>223</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>think possibly manage without one actually tum...</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>I am not sure to what limit his knowledge may ...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>sure limit knowledge may extend</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  num_words  \\\n",
       "0  id02310  Still, as I urged our leaving Ireland with suc...         19   \n",
       "1  id24541  If a fire wanted fanning, it could readily be ...         62   \n",
       "2  id00134  And when they had broken down the frail door t...         33   \n",
       "3  id27757  While I was thinking how I should possibly man...         41   \n",
       "4  id04081  I am not sure to what limit his knowledge may ...         11   \n",
       "\n",
       "   num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0                19        110              9                 3   \n",
       "1                49        330             33                 7   \n",
       "2                30        189             15                 3   \n",
       "3                34        223             19                 5   \n",
       "4                11         53              6                 1   \n",
       "\n",
       "   num_words_upper  num_words_title  max_word_len  \\\n",
       "0                1                3            11   \n",
       "1                1                3            11   \n",
       "2                0                1             9   \n",
       "3                2                3             9   \n",
       "4                1                1             9   \n",
       "\n",
       "                                        text_cleaned  \\\n",
       "0  still urge leave ireland inquietude impatience...   \n",
       "1  fire want fan could readily fan newspaper gove...   \n",
       "2  break frail door find two cleanly pick human s...   \n",
       "3  think possibly manage without one actually tum...   \n",
       "4                    sure limit knowledge may extend   \n",
       "\n",
       "   num_unique_words_cleaned  num_of_ADV  num_of_ADJ  \n",
       "0                        10           0           1  \n",
       "1                        27           0           2  \n",
       "2                        17           0           2  \n",
       "3                        20           2           4  \n",
       "4                         5           0           1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f98fff876d8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x= data[\"author\"], y = data[\"num_of_ADJ\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_matrix = vect.fit_transform(data[\"text_cleaned\"]) \n",
    "X_test_matrix = vect.transform(test[\"text_cleaned\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abased</th>\n",
       "      <th>abasement</th>\n",
       "      <th>abashed</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostation</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeute</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19308 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaem  ab  aback  abaft  abandon  abandonment  abaout  abased  abasement  \\\n",
       "0     0   0      0      0        0            0       0       0          0   \n",
       "1     0   0      0      0        0            0       0       0          0   \n",
       "2     0   0      0      0        0            0       0       0          0   \n",
       "3     0   0      0      0        0            0       0       0          0   \n",
       "4     0   0      0      0        1            0       0       0          0   \n",
       "\n",
       "   abashed  ...  æmilianus  æneid  ærial  æronaut  ærostation  æschylus  \\\n",
       "0        0  ...          0      0      0        0           0         0   \n",
       "1        0  ...          0      0      0        0           0         0   \n",
       "2        0  ...          0      0      0        0           0         0   \n",
       "3        0  ...          0      0      0        0           0         0   \n",
       "4        0  ...          0      0      0        0           0         0   \n",
       "\n",
       "   élite  émeute  οἶδα  υπνος  \n",
       "0      0       0     0      0  \n",
       "1      0       0     0      0  \n",
       "2      0       0     0      0  \n",
       "3      0       0     0      0  \n",
       "4      0       0     0      0  \n",
       "\n",
       "[5 rows x 19308 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vect.get_feature_names()\n",
    "df_X_train_matrix = pd.DataFrame(X_train_matrix.toarray(), columns=features)\n",
    "df_X_train_matrix.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaem</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abased</th>\n",
       "      <th>abasement</th>\n",
       "      <th>abashed</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostation</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeute</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19308 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaem  ab  aback  abaft  abandon  abandonment  abaout  abased  abasement  \\\n",
       "0     0   0      0      0        0            0       0       0          0   \n",
       "1     0   0      0      0        0            0       0       0          0   \n",
       "2     0   0      0      0        0            0       0       0          0   \n",
       "3     0   0      0      0        0            0       0       0          0   \n",
       "4     0   0      0      0        0            0       0       0          0   \n",
       "\n",
       "   abashed  ...  æmilianus  æneid  ærial  æronaut  ærostation  æschylus  \\\n",
       "0        0  ...          0      0      0        0           0         0   \n",
       "1        0  ...          0      0      0        0           0         0   \n",
       "2        0  ...          0      0      0        0           0         0   \n",
       "3        0  ...          0      0      0        0           0         0   \n",
       "4        0  ...          0      0      0        0           0         0   \n",
       "\n",
       "   élite  émeute  οἶδα  υπνος  \n",
       "0      0       0     0      0  \n",
       "1      0       0     0      0  \n",
       "2      0       0     0      0  \n",
       "3      0       0     0      0  \n",
       "4      0       0     0      0  \n",
       "\n",
       "[5 rows x 19308 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X_test_matrix = pd.DataFrame(X_test_matrix.toarray(), columns=features)\n",
    "df_X_test_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data.drop([\"id\",\"text\", \"text_cleaned\", \"author\"], axis = 1)\n",
    "\n",
    "df_train = pd.concat([data_df, df_X_train_matrix], axis=1)\n",
    "\n",
    "test_df = test.drop([\"id\",\"text\", \"text_cleaned\"], axis = 1)\n",
    "\n",
    "df_test = pd.concat([test_df, df_X_test_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_num</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>max_word_len</th>\n",
       "      <th>num_unique_words_clenaed</th>\n",
       "      <th>...</th>\n",
       "      <th>æmilianus</th>\n",
       "      <th>æneid</th>\n",
       "      <th>ærial</th>\n",
       "      <th>æronaut</th>\n",
       "      <th>ærostation</th>\n",
       "      <th>æschylus</th>\n",
       "      <th>élite</th>\n",
       "      <th>émeute</th>\n",
       "      <th>οἶδα</th>\n",
       "      <th>υπνος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>231</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>206</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>174</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19320 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   author_num  num_words  num_unique_words  num_chars  num_stopwords  \\\n",
       "0           0         41                35        231             19   \n",
       "1           1         14                14         71              8   \n",
       "2           0         36                32        200             16   \n",
       "3           2         34                32        206             13   \n",
       "4           1         27                25        174             11   \n",
       "\n",
       "   num_punctuations  num_words_upper  num_words_title  max_word_len  \\\n",
       "0                 7                2                3            12   \n",
       "1                 1                0                1             8   \n",
       "2                 5                0                1            13   \n",
       "3                 4                0                4             9   \n",
       "4                 4                0                2            14   \n",
       "\n",
       "   num_unique_words_clenaed  ...  æmilianus  æneid  ærial  æronaut  \\\n",
       "0                        21  ...          0      0      0        0   \n",
       "1                         6  ...          0      0      0        0   \n",
       "2                        18  ...          0      0      0        0   \n",
       "3                        20  ...          0      0      0        0   \n",
       "4                        16  ...          0      0      0        0   \n",
       "\n",
       "   ærostation  æschylus  élite  émeute  οἶδα  υπνος  \n",
       "0           0         0      0       0     0      0  \n",
       "1           0         0      0       0     0      0  \n",
       "2           0         0      0       0     0      0  \n",
       "3           0         0      0       0     0      0  \n",
       "4           0         0      0       0     0      0  \n",
       "\n",
       "[5 rows x 19320 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(\"author_num\", axis = 1)\n",
    "y = data['author_num']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8994949208330969\n",
      "0.8345250255362615\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf=MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "\n",
    "print (clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84       790\n",
      "           1       0.89      0.76      0.82       564\n",
      "           2       0.83      0.85      0.84       604\n",
      "\n",
      "    accuracy                           0.83      1958\n",
      "   macro avg       0.84      0.83      0.83      1958\n",
      "weighted avg       0.84      0.83      0.83      1958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_result=clf.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predicted_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss: 0.454 \n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict_proba(X_test)\n",
    "\n",
    "print (\"logloss: %0.3f \" % multiclass_logloss(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.403494</td>\n",
       "      <td>0.287808</td>\n",
       "      <td>0.308698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL       MWS\n",
       "0  id02310  0.403494  0.287808  0.308698\n",
       "1  id24541  0.403494  0.287808  0.308698\n",
       "2  id00134  0.403494  0.287808  0.308698\n",
       "3  id27757  0.403494  0.287808  0.308698\n",
       "4  id04081  0.403494  0.287808  0.308698"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(\"/kaggle/input/spooky-author-identification/sample_submission.csv\")\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_result = clf.predict_proba(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id02310</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>9.962615e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id24541</td>\n",
       "      <td>0.999579</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>2.406053e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id00134</td>\n",
       "      <td>0.236085</td>\n",
       "      <td>0.763903</td>\n",
       "      <td>1.182107e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27757</td>\n",
       "      <td>0.052882</td>\n",
       "      <td>0.947118</td>\n",
       "      <td>3.199258e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id04081</td>\n",
       "      <td>0.989892</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>5.130243e-03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       EAP       HPL           MWS\n",
       "0  id02310  0.002565  0.001174  9.962615e-01\n",
       "1  id24541  0.999579  0.000180  2.406053e-04\n",
       "2  id00134  0.236085  0.763903  1.182107e-05\n",
       "3  id27757  0.052882  0.947118  3.199258e-08\n",
       "4  id04081  0.989892  0.004978  5.130243e-03"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=pd.DataFrame()\n",
    "result[\"id\"]=test[\"id\"]\n",
    "result[\"EAP\"]=predicted_result[:,0]\n",
    "result[\"HPL\"]=predicted_result[:,1]\n",
    "result[\"MWS\"]=predicted_result[:,2]\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"submission_v3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
